{"cells":[{"cell_type":"code","source":["%fs ls /mnt/Fire_Department_Calls_for_Service"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["Note, you can also access the 1.6 GB of data directly from sfgov.org via this link: https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3"],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Using the SparkSession, create a DataFrame from the CSV file by inferring the schema:"],"metadata":{}},{"cell_type":"code","source":["fireServiceCallsDF = spark.read.csv('dbfs:/mnt/Fire_Department_Calls_for_Service/Fire_Department_Calls_for_Service.csv', header=True, inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Notice that the above cell takes ~15 seconds to run b/c it is inferring the schema by sampling the file and reading through it.\n\nInferring the schema works for ad hoc analysis against smaller datasets. But when working on multi-TB+ data, it's better to provide an **explicit pre-defined schema manually**, so there's no inferring cost:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Note that we are removing all space characters from the col names to prevent errors when writing to Parquet later\n\nfireSchema = StructType([StructField('CallNumber', IntegerType(), True),\n                     StructField('UnitID', StringType(), True),\n                     StructField('IncidentNumber', IntegerType(), True),\n                     StructField('CallType', StringType(), True),                  \n                     StructField('CallDate', StringType(), True),       \n                     StructField('WatchDate', StringType(), True),       \n                     StructField('ReceivedDtTm', StringType(), True),       \n                     StructField('EntryDtTm', StringType(), True),       \n                     StructField('DispatchDtTm', StringType(), True),       \n                     StructField('ResponseDtTm', StringType(), True),       \n                     StructField('OnSceneDtTm', StringType(), True),       \n                     StructField('TransportDtTm', StringType(), True),                  \n                     StructField('HospitalDtTm', StringType(), True),       \n                     StructField('CallFinalDisposition', StringType(), True),       \n                     StructField('AvailableDtTm', StringType(), True),       \n                     StructField('Address', StringType(), True),       \n                     StructField('City', StringType(), True),       \n                     StructField('ZipcodeofIncident', IntegerType(), True),       \n                     StructField('Battalion', StringType(), True),                 \n                     StructField('StationArea', StringType(), True),       \n                     StructField('Box', StringType(), True),       \n                     StructField('OriginalPriority', StringType(), True),       \n                     StructField('Priority', StringType(), True),       \n                     StructField('FinalPriority', IntegerType(), True),       \n                     StructField('ALSUnit', BooleanType(), True),       \n                     StructField('CallTypeGroup', StringType(), True),\n                     StructField('NumberofAlarms', IntegerType(), True),\n                     StructField('UnitType', StringType(), True),\n                     StructField('Unitsequenceincalldispatch', IntegerType(), True),\n                     StructField('FirePreventionDistrict', StringType(), True),\n                     StructField('SupervisorDistrict', StringType(), True),\n                     StructField('NeighborhoodDistrict', StringType(), True),\n                     StructField('Location', StringType(), True),\n                     StructField('RowID', StringType(), True)])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#Notice that no job is run this time\nfireServiceCallsDF = spark.read.csv('/FileStore/tables/Fire_Department_Calls_for_Service.csv', header=True, schema=fireSchema)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(fireServiceCallsDF.limit(5))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["fireServiceCallsDF.columns"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Count how many rows total there are in DataFrame (and see how long it takes to do a full scan from remote disk/S3):"],"metadata":{}},{"cell_type":"code","source":["fireServiceCallsDF.count()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["There are over 4.6 million rows in the DataFrame and it takes ~14 seconds to do a full read of it."],"metadata":{}},{"cell_type":"markdown","source":["DataFrames support two types of operations: *transformations* and *actions*.\n\nTransformations, like `select()` or `filter()` create a new DataFrame from an existing one.\n\nActions, like `show()` or `count()`, return a value with results to the user. Other actions like `save()` write the DataFrame to distributed storage (like S3 or HDFS)."],"metadata":{}},{"cell_type":"markdown","source":["####![Spark T/A](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pagecounts/trans_and_actions.png)"],"metadata":{}},{"cell_type":"markdown","source":["Transformations contribute to a query plan,  but  nothing is executed until an action is called."],"metadata":{}},{"cell_type":"markdown","source":["**Q-1) How many different types of calls were made to the Fire Department?**"],"metadata":{}},{"cell_type":"code","source":["# Use the .select() transformation to yank out just the 'Call Type' column, then call the show action\nfireServiceCallsDF.select('CallType').show(5)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Add the .distinct() transformation to keep only distinct rows\n# The False below expands the ASCII column width to fit the full text in the output\n\nfireServiceCallsDF.select('CallType').distinct().show(35,False)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["**Q-2) How many incidents of each call type were there?**"],"metadata":{}},{"cell_type":"code","source":["#Note that .count() is actually a transformation here\n\ndisplay(fireServiceCallsDF.select('CallType').groupBy('CallType').count().orderBy(\"count\", ascending=False))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Seems like the SF Fire department is called for medical incidents far more than any other type."],"metadata":{}},{"cell_type":"markdown","source":["** Date/Time Analysis**"],"metadata":{}},{"cell_type":"markdown","source":["**Q-3) How many years of Fire Service Calls is in the data file?**"],"metadata":{}},{"cell_type":"markdown","source":["Notice that the date or time columns are currently being interpreted as strings, rather than date or time objects:"],"metadata":{}},{"cell_type":"code","source":["fireServiceCallsDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Let's use the unix_timestamp() function to convert the string into a timestamp:\n\nhttps://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/api/python/pyspark.sql.html?highlight=spark#pyspark.sql.functions.from_unixtime"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Note that PySpark uses the Java Simple Date Format patterns\n\nfrom_pattern1 = 'MM/dd/yyyy'\nto_pattern1 = 'yyyy-MM-dd'\n\nfrom_pattern2 = 'MM/dd/yyyy hh:mm:ss aa'\nto_pattern2 = 'MM/dd/yyyy hh:mm:ss aa'\n\n\nfireServiceCallsTsDF = fireServiceCallsDF \\\n  .withColumn('CallDateTS', unix_timestamp(fireServiceCallsDF['CallDate'], from_pattern1).cast(\"timestamp\")) \\\n  .drop('CallDate') \\\n  .withColumn('WatchDateTS', unix_timestamp(fireServiceCallsDF['WatchDate'], from_pattern1).cast(\"timestamp\")) \\\n  .drop('WatchDate') \\\n  .withColumn('ReceivedDtTmTS', unix_timestamp(fireServiceCallsDF['ReceivedDtTm'], from_pattern2).cast(\"timestamp\")) \\\n  .drop('ReceivedDtTm') \\\n  .withColumn('EntryDtTmTS', unix_timestamp(fireServiceCallsDF['EntryDtTm'], from_pattern2).cast(\"timestamp\")) \\\n  .drop('EntryDtTm') \\\n  .withColumn('DispatchDtTmTS', unix_timestamp(fireServiceCallsDF['DispatchDtTm'], from_pattern2).cast(\"timestamp\")) \\\n  .drop('DispatchDtTm') \\\n  .withColumn('ResponseDtTmTS', unix_timestamp(fireServiceCallsDF['ResponseDtTm'], from_pattern2).cast(\"timestamp\")) \\\n  .drop('ResponseDtTm') \\\n  .withColumn('OnSceneDtTmTS', unix_timestamp(fireServiceCallsDF['OnSceneDtTm'], from_pattern2).cast(\"timestamp\")) \\\n  .drop('OnSceneDtTm') \\\n  .withColumn('TransportDtTmTS', unix_timestamp(fireServiceCallsDF['TransportDtTm'], from_pattern2).cast(\"timestamp\")) \\\n  .drop('TransportDtTm') \\\n  .withColumn('HospitalDtTmTS', unix_timestamp(fireServiceCallsDF['HospitalDtTm'], from_pattern2).cast(\"timestamp\")) \\\n  .drop('HospitalDtTm') \\\n  .withColumn('AvailableDtTmTS', unix_timestamp(fireServiceCallsDF['AvailableDtTm'], from_pattern2).cast(\"timestamp\")) \\\n  .drop('AvailableDtTm')  "],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["fireServiceCallsTsDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Notice that the formatting of the timestamps is now different:"],"metadata":{}},{"cell_type":"code","source":["display(fireServiceCallsTsDF.limit(5))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Finally calculate how many distinct years of data is in the CSV file:"],"metadata":{}},{"cell_type":"code","source":["fireServiceCallsTsDF.select(year('CallDateTS')).distinct().orderBy('year(CallDateTS)').show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["**Q-4) How many service calls were logged in the past 7 days?**"],"metadata":{}},{"cell_type":"markdown","source":["Note that  July 4th, is the 185th day of the year.\n\nFilter the DF down to just 2017 and days of year greater than 180:"],"metadata":{}},{"cell_type":"code","source":["fireServiceCallsTsDF.filter(year('CallDateTS') == '2017').filter(dayofyear('CallDateTS') >= 180).select(dayofyear('CallDateTS')).distinct().orderBy('dayofyear(CallDateTS)').show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["fireServiceCallsTsDF.filter(year('CallDateTS') == '2017').filter(dayofyear('CallDateTS') >= 180).groupBy(dayofyear('CallDateTS')).count().orderBy('dayofyear(CallDateTS)').show()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Note above that July 4th, 2017 was the 185th day of the year."],"metadata":{}},{"cell_type":"markdown","source":["Visualize the results in a bar graph:"],"metadata":{}},{"cell_type":"code","source":["display(fireServiceCallsTsDF.filter(year('CallDateTS') == '2017').filter(dayofyear('CallDateTS') >= 180).groupBy(dayofyear('CallDateTS')).count().orderBy('dayofyear(CallDateTS)'))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["https://www.cbsnews.com/news/san-francisco-has-second-straight-day-of-triple-digit-temperatures/"],"metadata":{}},{"cell_type":"markdown","source":["The DataFrame is currently comprised of 13 partitions:"],"metadata":{}},{"cell_type":"code","source":["fireServiceCallsTsDF.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["![Partitions](http://curriculum-release.s3-website-us-west-2.amazonaws.com/sf_open_data_meetup/df_13_parts.png)"],"metadata":{}},{"cell_type":"code","source":["fireServiceCallsTsDF.repartition(6).createOrReplaceTempView(\"fireServiceVIEW\");"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["spark.catalog.cacheTable(\"fireServiceVIEW\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["# Call .count() to materialize the cache\nspark.table(\"fireServiceVIEW\").count()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["fireServiceDF = spark.table(\"fireServiceVIEW\")"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# Note that the full scan + count in memory takes < 1 second!\n\nfireServiceDF.count()"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["spark.catalog.isCached(\"fireServiceVIEW\")"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["The 6 partitions are now cached in memory:"],"metadata":{}},{"cell_type":"markdown","source":["![6 Partitions](http://curriculum-release.s3-website-us-west-2.amazonaws.com/sf_open_data_meetup/df_6_parts.png)"],"metadata":{}},{"cell_type":"markdown","source":["Use the Spark UI to see the 6 partitions in memory:"],"metadata":{}},{"cell_type":"markdown","source":["![Mem UI](http://curriculum-release.s3-website-us-west-2.amazonaws.com/sf_open_data_meetup/mem_ui.png)"],"metadata":{}},{"cell_type":"markdown","source":["Now that our data has the correct date types for each column and it is correctly partitioned, let's write it down as a parquet file for future loading:"],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["fireServiceDF.write.format('parquet').save('/mnt/Fire_Department_Calls_for_Service2/')"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["Now the directory should contain 6 .gz compressed Parquet files (one for each partition):"],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/Fire_Department_Calls_for_Service2/"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["Here's how you can easily read the parquet file from S3 in the future:"],"metadata":{}},{"cell_type":"code","source":["tempDF = spark.read.parquet('/mnt/Fire_Department_Calls_for_Service2/')"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["display(tempDF.limit(2))"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["**SQL Queries**"],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"SELECT count(*) FROM fireServiceVIEW\").show()"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["%sql SELECT count(*) FROM fireServiceVIEW;"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["'Spark Jobs' in the cell above to see that 7 tasks were launched to run the count... 6 tasks to reach the data from each of the 6 partitions and do a pre-aggregation on each partition, then a final task to aggregate the count from all 6 tasks:"],"metadata":{}},{"cell_type":"markdown","source":["![Job details](http://curriculum-release.s3-website-us-west-2.amazonaws.com/sf_open_data_meetup/6_tasks.png)"],"metadata":{}},{"cell_type":"markdown","source":["You can use the Spark Stages UI to see the 6 tasks launched in the middle stage:"],"metadata":{}},{"cell_type":"markdown","source":["![Event Timeline](http://curriculum-release.s3-website-us-west-2.amazonaws.com/sf_open_data_meetup/event_timeline.png)"],"metadata":{}},{"cell_type":"markdown","source":["**Q-5) Which neighborhood in SF generated the most calls last year?**"],"metadata":{}},{"cell_type":"code","source":["%sql SELECT `NeighborhoodDistrict`, count(`NeighborhoodDistrict`) AS Neighborhood_Count FROM fireServiceVIEW WHERE year(`CallDateTS`) == '2015' GROUP BY `NeighborhoodDistrict` ORDER BY Neighborhood_Count DESC LIMIT 15;"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["Expand the Spark Job details in the cell above and notice that the last stage uses 200 partitions! This is default is non-optimal, given that we only have ~1.6 GB of data and 3 slots.\n\nChange the shuffle.partitions option to 6:"],"metadata":{}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", 6)"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["Re-run the same SQL query and notice the speed increase:"],"metadata":{}},{"cell_type":"code","source":["%sql SELECT `NeighborhoodDistrict`, count(`NeighborhoodDistrict`) AS Neighborhood_Count FROM fireServiceVIEW WHERE year(`CallDateTS`) == '2015' GROUP BY `NeighborhoodDistrict` ORDER BY Neighborhood_Count DESC LIMIT 15;"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":["SQL also has some handy commands like `DESC` (describe) to see the schema + data types for the table:"],"metadata":{}},{"cell_type":"code","source":["%sql DESC fireServiceVIEW;"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":["** DataFrame Joins**"],"metadata":{}},{"cell_type":"markdown","source":["**Q-6) What was the primary non-medical reason most people called the fire department from the Tenderloin last year?**"],"metadata":{}},{"cell_type":"markdown","source":["The \"Fire Incidents\" data includes a summary of each (non-medical) incident to which the SF Fire Department responded."],"metadata":{}},{"cell_type":"markdown","source":["Let's do a join to the Fire Incidents data on the \"Incident Number\" column:\n\nhttps://data.sfgov.org/Public-Safety/Fire-Incidents/wr8u-xric"],"metadata":{}},{"cell_type":"markdown","source":["Read the Fire Incidents CSV file into a DataFrame:"],"metadata":{}},{"cell_type":"code","source":["incidentsDF = spark.read.csv('/mnt/Fire_Department_Calls_for_Service/Fire_Incidents.csv', header=True, inferSchema=True).withColumnRenamed('Incident Number', 'IncidentNumber').cache()"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["incidentsDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["# Materialize the cache\nincidentsDF.count()"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["display(incidentsDF.limit(3))"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["joinedDF = fireServiceDF.join(incidentsDF, fireServiceDF.IncidentNumber == incidentsDF.IncidentNumber)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["display(joinedDF.limit(3))"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["#Note that the joined DF is only 1.1 million rows b/c we did an inner join (the original Fire Service Calls data had 4+ million rows)\njoinedDF.count()"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["joinedDF.filter(year('CallDateTS') == '2017').filter(col('NeighborhoodDistrict') == 'Tenderloin').count()"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["display(joinedDF.filter(year('CallDateTS') == '2017').filter(col('NeighborhoodDistrict') == 'Tenderloin').groupBy('Primary Situation').count().orderBy(desc(\"count\")).limit(10))"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":["Most of the calls were False Alarms!"],"metadata":{}},{"cell_type":"markdown","source":["What do residents of Russian Hill call the fire department for?"],"metadata":{}},{"cell_type":"code","source":["display(joinedDF.filter(year('CallDateTS') == '2017').filter(col('NeighborhoodDistrict') == 'Russian Hill').groupBy('Primary Situation').count().orderBy(desc(\"count\")).limit(10))"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"markdown","source":["** Convert a Spark DataFrame to a Pandas DataFrame **"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nfrom sklearn import tree\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\n"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["p17 = joinedDF.filter(year('CallDateTS') == '2017').toPandas()"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["p17.dtypes"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["p17.columns\ncolumns = ['CallType', 'CallFinalDisposition',  'StationArea', 'OriginalPriority', 'Priority', 'FinalPriority', 'ALSUnit','CallTypeGroup', 'NumberofAlarms', 'UnitType', 'EMS Units', 'Ignition Cause']\np17 = p17[columns]\np17.columns"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["cleanup = {}\nfor column in columns :\n    cleanup = {}\n    temp = list(set(p17[column]))\n    temp_map = {temp[i]:i for i in range(len(temp))}\n    cleanup[column] = temp_map\n    print(temp_map)\n    print(column)\n    try:\n      p17.replace(cleanup, inplace=True)\n    except:\n      continue\np17['ALSUnit'] = p17['ALSUnit'].astype(int)"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["\ncolumns = ['CallType', 'CallFinalDisposition',  'StationArea', 'OriginalPriority', 'Priority', 'FinalPriority', 'CallTypeGroup', 'NumberofAlarms', 'UnitType', 'EMS Units', 'Ignition Cause']\nX = p17[columns]\nY = p17['ALSUnit']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint precision_score(y_test, y_pred)\nprint accuracy_score(y_test,y_pred)\n"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["columns = ['UnitType', 'CallFinalDisposition',  'StationArea', 'OriginalPriority', 'Priority', 'FinalPriority', 'CallTypeGroup', 'NumberofAlarms', 'ALSUnit', 'EMS Units', 'Ignition Cause']\nX = p17[columns]\nY = p17['CallType']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\nclf = svm.SVC()\nclf = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprecision_score(y_test, y_pred)"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["columns = ['CallType', 'CallFinalDisposition',  'StationArea', 'OriginalPriority', 'Priority', 'FinalPriority', 'CallTypeGroup', 'NumberofAlarms', 'ALSUnit', 'EMS Units', 'Ignition Cause']\nX = p17[columns]\nY = p17['UnitType']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\nneigh = KNeighborsClassifier(n_neighbors=10)\nneigh.fit(X_train, y_train) \ny_pred = neigh.predict(X_test)\nprint precision_score(y_test, y_pred)\nprint accuracy_score(y_test,y_pred)"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["columns = ['CallType', 'CallFinalDisposition',  'StationArea', 'OriginalPriority', 'Priority', 'FinalPriority', 'CallTypeGroup', 'NumberofAlarms', 'ALSUnit', 'EMS Units', 'Ignition Cause', 'UnitType']\nX = p17[columns]\nY = p17['UnitType']\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n# p17[columns]=p17[columns].as_matrix()\nkmeans = KMeans(n_clusters=3, random_state=0).fit(X_train,y_train)\npred= kmeans.predict(X_test)\nkmeans.cluster_centers_"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["p17.describe()"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":110}],"metadata":{"name":"FireBia","notebookId":2222189852935138},"nbformat":4,"nbformat_minor":0}
